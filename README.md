# Parallax File Finder

> **AI-Powered Local File Search with Decentralized Inference**  
> A blazingly fast, intelligent file search system that leverages Parallax for distributed model inference to find exactly what you're looking for across hundreds of thousands of files.

---

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [Solution](#solution)
3. [Key Features](#key-features)
4. [Architecture](#architecture)
5. [How It Works](#how-it-works)
6. [Installation & Setup](#installation--setup)
7. [Usage Guide](#usage-guide)
8. [Search Modes Explained](#search-modes-explained)
9. [Performance & Scaling](#performance--scaling)
10. [Configuration](#configuration)
11. [Troubleshooting](#troubleshooting)

---

## Problem Statement

### The Challenge

Finding relevant files in large codebases, document archives, or research repositories is slow and frustrating:

- **Keyword search is brittle** â€” typos, synonyms, and context-dependent meaning cause misses
- **Scale kills productivity** â€” searching 100K+ files with traditional tools takes minutes
- **No semantic understanding** â€” traditional indexers don't grasp *intent*, only keywords
- **Centralized APIs are expensive & slow** â€” cloud-based search costs money and adds latency

### Real-World Scenarios

- Developer searching for a bug fix across 500+ files
- Researcher finding relevant papers from a local archive
- Data analyst locating specific data transformations in a legacy codebase
- Legal team searching contract clauses across thousands of documents

---

## Solution

**Parallax File Finder** combines:

1. **Local TF-IDF Semantic Search** â€” Fast pre-filtering of candidates without hitting the API
2. **Distributed Model Inference (Parallax)** â€” Runs LLMs locally across multiple devices with zero cloud dependency
3. **Hybrid + Full Search Modes** â€” Choose speed or comprehensiveness based on your needs
4. **Smart Batching** â€” Handles 10K+ files efficiently by splitting work into manageable chunks

**Result:** Find relevant files in seconds with semantic understanding, all running on your own hardware.

---

## Key Features

### âœ¨ Hybrid Search (Fast Mode)
- Local TF-IDF semantic ranking â†’ Top 100 candidates
- Send only relevant subset to model â†’ Saves tokens & time
- Perfect for: Quick searches, large file sets, budget-conscious workflows
- **Typical latency:** 5â€“15 seconds for 10K files

### ğŸ” Full AI Search (Comprehensive Mode)
- All files sent to model for deep analysis
- Model considers complete context across the entire dataset
- Perfect for: Finding subtle patterns, ensuring nothing is missed, smaller file sets
- **Typical latency:** 30â€“120 seconds (depends on file count & model speed)

### ğŸ“¦ Intelligent File Indexing
- Scans and caches files (400-char preview + full content)
- Save/load indexes for faster re-searches
- Supports: `.txt`, `.md`, `.log`, `.py`, `.json`, `.csv`, `.js`, `.html`, `.css`
- Handles encoding errors gracefully

### âš¡ Distributed Inference with Parallax
- No cloud dependency â€” runs on your devices
- Pipeline-parallel model sharding across nodes
- Continuous batching for high throughput
- Supports Qwen, Llama, DeepSeek, and other models

### ğŸ¨ Native Desktop GUI (wxPython)
- Cross-platform (Windows, macOS, Linux)
- Real-time activity logging
- Dark theme for extended use
- Responsive even with large result sets

---

## Architecture

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Parallax File Finder                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   UI Layer (wxPython)                    â”‚  â”‚
â”‚  â”‚  â€¢ File browsing & indexing controls                    â”‚  â”‚
â”‚  â”‚  â€¢ Search input & mode selection                        â”‚  â”‚
â”‚  â”‚  â€¢ Results display & file opening                       â”‚  â”‚
â”‚  â”‚  â€¢ Activity logging                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚               Application Core Layer                      â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  FileIndexer     â”‚      â”‚   SearchEngine           â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Scan files    â”‚      â”‚  â€¢ Hybrid Search Logic   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Extract text  â”‚      â”‚  â€¢ TF-IDF ranking        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Cache index   â”‚      â”‚  â€¢ Batch orchestration   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Save/Load     â”‚      â”‚                          â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          Parallax Client (Local Inference)               â”‚  â”‚
â”‚  â”‚  â€¢ Streaming responses (OpenAI-compatible API)          â”‚  â”‚
â”‚  â”‚  â€¢ Batched requests (300 files per batch)               â”‚  â”‚
â”‚  â”‚  â€¢ Error handling & retry logic                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚        Parallax Cluster (Distributed LLM Serving)        â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Scheduler    â”‚  â”‚ Worker Node  â”‚  â”‚ Worker Node  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ (3001)       â”‚â†’ â”‚ (GPU/CPU)    â”‚  â”‚ (GPU/CPU)    â”‚   â”‚  â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  â€¢ Model: Qwen/Qwen3, Llama, DeepSeek, etc.             â”‚  â”‚
â”‚  â”‚  â€¢ Pipeline Parallelism: Split model across GPUs        â”‚  â”‚
â”‚  â”‚  â€¢ KV Cache Management: Dynamic allocation              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

**Hybrid Search Path:**
```
User Query
    â†“
[Local TF-IDF Ranking] â† 100 top candidates selected
    â†“
[Batch Preparation] â† Files grouped into chunks
    â†“
[Parallax Model] â† Model ranks and filters candidates
    â†“
[JSON Parsing] â† Extract ranked IDs + reasoning
    â†“
[Results Display] â† Show top N matches to user
```

**Full Search Path:**
```
User Query
    â†“
[Batch Splitting] â† 10K files â†’ 34 batches (300 each)
    â†“
[Parallel Batching] â† Up to 3 concurrent batch requests
    â†“
[Parallax Model] â† Each batch processed independently
    â†“
[Result Merging] â† Combine & sort all results by score
    â†“
[Results Display] â† Top N ranked matches shown
```

---

## How It Works

### Step 1: File Indexing

```python
# FileIndexer scans your directory
indexer = FileIndexer()
indexer.set_root_path("/path/to/files")
files = indexer.index_files(progress_callback=update_ui)

# Each file gets:
# - Full content (up to 200K chars)
# - 400-char preview
# - Path, name, extension, size, modified time
```

**What happens:**
- Walks directory tree recursively
- Filters by allowed extensions
- Reads file contents with encoding fallback
- Stores in memory for fast searching
- Can be saved/loaded as JSON for persistence

### Step 2: Hybrid Search (Recommended)

```python
# TF-IDF semantic ranking
top_100 = search_engine.semantic_search(query, indexed_files, top_k=100)

# Send subset to model
results = search_engine.ai_search_hybrid(query, indexed_files)
```

**What happens:**
1. **TF-IDF Vectorization** â€” Converts query + all file previews into vectors
2. **Cosine Similarity** â€” Ranks files by relevance score
3. **Top-K Selection** â€” Takes top 100 candidates
4. **Model Refinement** â€” Sends only these 100 to Parallax for final ranking
5. **JSON Parsing** â€” Model returns ranked list + reasoning

**Why hybrid is faster:**
- TF-IDF is instant (< 1 second for 10K files)
- Model only processes 100 files instead of 10K
- Saves 90% of token usage
- Still catches files TF-IDF might miss due to synonyms/context

### Step 3: Full AI Search (Comprehensive)

```python
# When you need to be thorough
results = search_engine.ai_search_full(query, indexed_files)
```

**What happens:**
1. **Batching** â€” Splits 10K files into chunks of 300
2. **Parallel Processing** â€” Sends up to 3 batches concurrently
3. **Per-Batch Ranking** â€” Model ranks each batch independently
4. **Merging** â€” Combines results and sorts by score
5. **Retry Logic** â€” Handles failures with exponential backoff

**Why full search is thorough:**
- Model sees all files (not just top 100 from TF-IDF)
- Catches edge cases & subtle patterns
- Better for small-to-medium datasets (< 50K files)
- Use when correctness > speed

### Step 4: Result Display

Each result shows:
- **Rank** (#1, #2, etc.)
- **File name** (with type info)
- **File path** (full location)
- **Preview** (first 150 chars)
- **Click to open** â€” Opens file in default app

Plus: **AI Reasoning** â€” Model's explanation of why these files match

---

## Installation & Setup

### Prerequisites

- **Python 3.8+**
- **Parallax cluster running locally** (or accessible over network)
- **wxPython** (GUI framework)
- **scikit-learn** (for TF-IDF)
- **requests** (for HTTP communication)

### Step 1: Clone Repository

```bash
git clone https://github.com/LonelyGuy-SE1/PARALLAX-HACKATHON-A.git
cd "PARALLAX HACKATHON A"
```

### Step 2: Create Virtual Environment (Recommended)

```bash
python -m venv .venv

# On Windows:
.venv\Scripts\activate

# On macOS/Linux:
source .venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**requirements.txt includes:**
```
wxPython>=4.2.0
requests>=2.28.0
scikit-learn>=1.0.0
```

### Step 4: Start Parallax Cluster

#### Option A: Scheduler + Worker (Local)

**Terminal 1 â€” Start Scheduler:**
```bash
parallax run -m Qwen/Qwen3-0.6B -n 1
```

Expected output:
```
[INFO] Scheduler started on http://localhost:3001
[INFO] Waiting for workers to join...
```

**Terminal 2 â€” Join as Worker:**
```bash
parallax join
```

Expected output:
```
[INFO] Connected to scheduler
[INFO] Model loaded on GPU
```

#### Option B: Scheduler Only (if running on same machine)

```bash
parallax run -m Qwen/Qwen3-0.6B
```

#### Option C: Remote Setup

**On scheduler machine:**
```bash
parallax run --host 0.0.0.0 -m Qwen/Qwen3-0.6B
```

**On worker machines:**
```bash
parallax join -s <scheduler-ip>:9090
```

### Step 5: Verify Parallax Endpoint

```bash
curl -X POST http://localhost:3001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "hello"}],
    "max_tokens": 100,
    "stream": true
  }'
```

Should return streaming response with `data: {...}` lines.

### Step 6: Run Parallax File Finder

```bash
python ui_main.py
```

GUI window opens â†’ Ready to search!

---

## Usage Guide

### Basic Workflow

#### 1. Select a Folder
- Click **BROWSE** button
- Select folder containing files to search
- Path appears in text box

#### 2. Index Files
- Click **INDEX** button
- Watch activity log for progress
- Status updates show file count
- Once complete: "Indexed 1,234 files. Ready."

#### 3. Type Your Query
- Click search box
- Type natural language query
  - âœ… *"Find database connection code"*
  - âœ… *"Where is the user authentication?"*
  - âœ… *"Show me error handling examples"*
  - âœ… *"Find deprecated function calls"*

#### 4. Choose Search Mode

**Hybrid Search (Fast)** â€” Default for most use cases
- Click button or press Enter
- Returns in 5â€“15 seconds
- Good for: Quick exploration, large datasets

**Full AI Search (Slow)** â€” When you need comprehensive results
- Takes 30â€“120 seconds
- Searches entire index
- Good for: Important queries, small-to-medium datasets

#### 5. Review Results
- Results appear in order of relevance
- Click any result to open file
- Read AI reasoning at top of results
- Iterate with new queries

### Advanced Features

#### Save & Load Indexes

**Save current index:**
- Click **SAVE INDEX** button
- Choose location & filename
- Creates `.json` file of all indexed files

**Load saved index:**
- Click **LOAD INDEX** button
- Select `.json` file
- Instantly available for searching
- No re-scanning needed

**Why use this?**
- Indexing 50K files takes 2â€“5 minutes
- Save index, then search it instantly later
- Perfect for large static datasets

#### Activity Log

The activity log at bottom of window shows:
- When indexing started/completed
- Search mode and query text
- Background worker status
- Results count and timing
- Any errors encountered

Useful for: Debugging, understanding performance, auditing

---

## Search Modes Explained

### ğŸš€ Hybrid Search (Recommended Default)

**How it works:**
1. TF-IDF locally ranks all files â†’ picks top 100
2. Sends top 100 to Parallax model
3. Model re-ranks and filters
4. Returns final results with reasoning

**When to use:**
- âœ… Large datasets (1Kâ€“1M files)
- âœ… Speed is important
- âœ… You trust TF-IDF + model together
- âœ… Token budget is limited

**Pros:**
- Very fast (5â€“15 sec for 10K files)
- Low token usage
- Still semantically accurate
- Great for interactive exploration

**Cons:**
- Might miss files if TF-IDF pre-filter is too aggressive
- Only considers top 100 from TF-IDF
- Less comprehensive than full search

**Token usage example (10K files):**
- Top 100 candidates sent â‰ˆ 15â€“30K tokens per search
- Full AI search would use â‰ˆ 200â€“500K tokens

---

### ğŸ”¬ Full AI Search (Comprehensive)

**How it works:**
1. Splits all files into batches (300 files each)
2. Sends batches in parallel (up to 3 at a time)
3. Model ranks each batch independently
4. Results merged and sorted by score
5. Returns all matches (not just top 100)

**When to use:**
- âœ… Small-to-medium datasets (< 50K files)
- âœ… Accuracy > speed
- âœ… Finding subtle patterns
- âœ… You want no stone left unturned

**Pros:**
- Comprehensive â€” nothing missed
- Model sees all context
- Better for edge cases
- More reliable for important searches

**Cons:**
- Slower (30â€“120 sec for 10K files)
- Higher token usage
- Not practical for 1M+ file sets
- Can time out on very large datasets

**Batching explanation:**
```
Full Search Flow (10K files):

10,000 files â†’ Batch 1 (files 0â€“300)
            â†’ Batch 2 (files 300â€“600)
            â†’ ...
            â†’ Batch 34 (files 9,900â€“10K)

Parallel processing (3 workers):
Time 0s:   Batch 1, 2, 3 start
Time 20s:  Batch 1, 2, 3 done â†’ Batch 4, 5, 6 start
Time 40s:  Batch 4, 5, 6 done â†’ Batch 7, 8, 9 start
...
Time 220s: All batches complete, results merged
```

---

## Performance & Scaling

### File Count Handling

| File Count | Hybrid Time | Full Time | Recommended Mode |
|-----------|-----------|----------|-----------------|
| 100       | 3â€“5 sec   | 5â€“10 sec | Either         |
| 1,000     | 4â€“8 sec   | 15â€“30 sec| Hybrid         |
| 10,000    | 5â€“15 sec  | 60â€“120 sec| Hybrid       |
| 100,000   | 8â€“20 sec  | N/A*    | Hybrid only     |
| 1,000,000 | 10â€“25 sec | N/A*    | Hybrid only     |

*Full search becomes impractical due to token limits and latency.

### Batch Size & Parallelization

**Current settings (tunable in `search_engine.py`):**
- Batch size: **300 files**
- Max concurrent batches: **3**
- Retry attempts: **2**
- Exponential backoff: **1s â†’ 2s â†’ 4s**

**Why these numbers?**
- 300 files â‰ˆ 50â€“80K tokens (fits in model context)
- 3 parallel batches = good balance (doesn't overwhelm scheduler)
- 2 retries catches temporary network issues
- Exponential backoff prevents hammering server

### Memory Usage

**Per-file overhead:** ~5â€“10 KB (depending on file size)
- Example: 100K files â‰ˆ 500 MBâ€“1 GB in memory

**Optimization:** Save indexes to disk, load only when needed

### Model Performance

Search speed depends heavily on:
1. **Model size** â€” Qwen3-0.6B (fast) vs Qwen3-235B (comprehensive)
2. **Hardware** â€” GPU (fast) vs CPU (slow)
3. **File count** â€” More files = more processing
4. **Query complexity** â€” Complex queries take longer

**Typical latencies by model:**

| Model                | Batch Size | Time/Batch |
|-------------------|-----------|-----------|
| Qwen3-0.6B (GPU)  | 300       | 15â€“25 sec |
| Qwen3-7B (GPU)    | 300       | 20â€“40 sec |
| Llama2-70B (GPU)  | 300       | 30â€“60 sec |
| Any (CPU)         | 300       | 5â€“10 min  |

---

## Configuration

### config.py

```python
# Parallax Endpoint
PARALLAX_API_URL = "http://localhost:3001/v1/chat/completions"
PARALLAX_TIMEOUT = 120  # seconds

# Indexing
PREVIEW_CHARS = 400        # Characters shown in result preview
MAX_FILE_CHARS = 200_000   # Max file content to cache

# Allowed file types
ALLOWED_EXTENSIONS = {
    '.txt', '.md', '.log',  # Text
    '.py', '.js', '.json',  # Code
    '.csv', '.html', '.css' # Data/markup
}

# UI Theme (Dark mode)
THEME = {
    "bg": "#000000",           # Main background
    "panel_bg": "#1A1A1A",     # Panel/card background
    "text": "#FFFFFF",         # Primary text
    "text_dim": "#999999",     # Secondary text
    "accent": "#00D9FF",       # Highlight color
}

FONT_FAMILY = "Segoe UI"
```

### Customize Search Behavior

**In `search_engine.py`:**

```python
# Hybrid search: adjust TF-IDF top-k
def ai_search_hybrid(self, query, files, top_k=100, max_results=20):
    # top_k: how many candidates to send to model (100 default)
    # max_results: how many to return (20 default)

# Full search: adjust batching
batch_size = 300      # Files per batch (increase for more speed)
max_workers = 3       # Concurrent requests (increase for parallelism)
```

### Remote Parallax Setup

If running Parallax on a different machine:

```python
# config.py
PARALLAX_API_URL = "http://<scheduler-ip>:3001/v1/chat/completions"
```

Example:
```python
PARALLAX_API_URL = "http://192.168.1.100:3001/v1/chat/completions"
```

---

## Troubleshooting

### "Connection refused" / "Cannot connect to Parallax"

**Problem:** Parallax cluster not running or not reachable

**Solutions:**
1. Check Parallax is running:
   ```bash
   curl http://localhost:3001/v1/chat/completions
   ```

2. Restart Parallax:
   ```bash
   # Terminal 1
   parallax run -m Qwen/Qwen3-0.6B
   
   # Terminal 2
   parallax join
   ```

3. If remote, verify IP connectivity

### Slow Search / Timeouts

**Problem:** Searches taking too long or timing out

**Solutions:**
1. **Increase timeout** in `config.py`:
   ```python
   PARALLAX_TIMEOUT = 300  # 5 minutes
   ```

2. **Use Hybrid search** â€” Much faster than Full

3. **Check Parallax performance** â€” GPU usage, model loading

---

## Model Selection

### Recommended Models

| Model | Size | Speed | Best For |
|-------|------|-------|----------|
| Qwen3-0.6B | 0.6B | âš¡âš¡âš¡ | Fast exploration |
| Qwen3 | 3B | âš¡âš¡ | Balanced (recommended) |
| Llama2-7B | 7B | âš¡ | High accuracy |
| Llama2-70B | 70B | ğŸ¢ | Maximum accuracy |

**For this hackathon:**
```bash
parallax run -m Qwen/Qwen3-0.6B -n 1  # Fast, good enough
```

---

## Future Improvements

- [ ] Re-ranking algorithm for better top-N selection
- [ ] Query expansion (synonyms, related terms)
- [ ] Caching of model responses for identical queries
- [ ] Support for binary files (PDF, DOCX via OCR)
- [ ] Web interface alternative to wxPython
- [ ] Elasticsearch integration for very large datasets

---

## License

Apache 2.0 â€” See LICENSE file

---

## Contributors & Credits

Built with â¤ï¸ for the Parallax Hackathon by **LonelyGuy-SE1**

**Stack:**
- **Parallax** (https://github.com/GradientHQ/parallax) â€” Distributed LLM inference
- **wxPython** â€” Cross-platform GUI
- **scikit-learn** â€” TF-IDF vectorization
- **Qwen3** â€” LLM backbone

---

## Support & Questions

1. **GitHub Issues:** Report bugs
2. **Parallax Discord:** https://discord.gg/parallax
3. **PRs Welcome!**

---

**Happy searching! ğŸ”âœ¨**
